{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKhLnso5C5R1UquYEf7jKd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ojas-Mahajan/Multilingual-Mobile-App-Review-Analysis/blob/main/Copy_of_Multilingual_Mobile_App_Review_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIVVCLt-Kp3a"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch"
      ],
      "metadata": {
        "id": "cCMn4k8FdyYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your cleaned and prepared dataset\n",
        "df = pd.read_csv('cleaned_reviews.csv')\n",
        "\n",
        "# Drop any potential rows that might still have missing text\n",
        "df = df.dropna(subset=['review_text'])\n",
        "\n",
        "# Display the first few rows to confirm it's loaded correctly\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "FGhB29GbeCh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert pandas DataFrame to Hugging Face Dataset object\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "train_test_split_dataset = dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split_dataset['train']\n",
        "test_dataset = train_test_split_dataset['test']\n",
        "\n",
        "print(\"Training data shape:\", train_dataset.shape)\n",
        "print(\"Testing data shape:\", test_dataset.shape)"
      ],
      "metadata": {
        "id": "WyRueKgmeMoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model we are using\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create a function to tokenize the text\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['review_text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply the tokenizer to our datasets\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "lOnfRXJneQLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# --- Assuming previous code for loading and tokenizing data is here ---\n",
        "# tokenized_train_dataset, tokenized_test_dataset, model_name are defined\n",
        "\n",
        "# Clear CUDA cache to free up unused memory from previous runs\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "# Define a function to compute metrics during evaluation\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Map sentiment labels to integers\n",
        "sentiment_to_id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "def map_sentiment_to_labels(examples):\n",
        "    examples[\"labels\"] = sentiment_to_id[examples[\"sentiment\"]]\n",
        "    return examples\n",
        "\n",
        "# Apply the mapping to create the 'labels' column\n",
        "tokenized_train_dataset = tokenized_train_dataset.map(map_sentiment_to_labels)\n",
        "tokenized_test_dataset = tokenized_test_dataset.map(map_sentiment_to_labels)\n",
        "\n",
        "# Define the training arguments with more aggressive optimizations\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,   # <-- FURTHER REDUCED BATCH SIZE\n",
        "    per_device_eval_batch_size=4,    # <-- FURTHER REDUCED EVAL BATCH SIZE\n",
        "    gradient_accumulation_steps=8,   # <-- INCREASED ACCUMULATION\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Create the Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Start the training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "S954gfZ2eYUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the final evaluation on the test dataset\n",
        "evaluation_results = trainer.evaluate()\n",
        "\n",
        "# Print the results\n",
        "print(\"Evaluation Results:\", evaluation_results)"
      ],
      "metadata": {
        "id": "nqb4Xc2ZHlFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predictions for the entire test set\n",
        "predictions = trainer.predict(tokenized_test_dataset)\n",
        "\n",
        "# The output predictions are logits, so we need to find the class with the highest score\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# You can now compare the predicted_labels with the actual labels\n",
        "# predictions.label_ids contains the true labels"
      ],
      "metadata": {
        "id": "VjjpAoSeSSzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a new review to classify\n",
        "new_review = \"This app is absolutely fantastic, I use it every day!\"\n",
        "\n",
        "# Tokenize the new text\n",
        "inputs = tokenizer(new_review, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Move tensors to the same device as the model (important for GPU)\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# Get model output (logits)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get the predicted class index\n",
        "predicted_class_idx = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "# You'll need a mapping from index to label name\n",
        "# Assuming 0: negative, 1: neutral, 2: positive (this depends on your data's encoding)\n",
        "# Let's create it from the original dataframe\n",
        "label_mapping = {i: label for i, label in enumerate(df['sentiment'].astype('category').cat.categories)}\n",
        "\n",
        "print(f\"New Review: '{new_review}'\")\n",
        "print(f\"Predicted Sentiment: {label_mapping[predicted_class_idx]}\")"
      ],
      "metadata": {
        "id": "hBCiA626SX_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get the true labels and predicted labels from the previous step\n",
        "true_labels = predictions.label_ids\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# Get the class names from our label mapping\n",
        "class_names = list(label_mapping.values())\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y5dKi1eXSd-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AH2a1g0LS13H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
